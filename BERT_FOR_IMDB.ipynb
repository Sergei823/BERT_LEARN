{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install pytorch-pretrained-bert","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:18:55.309040Z","iopub.execute_input":"2023-07-25T02:18:55.309634Z","iopub.status.idle":"2023-07-25T02:19:11.153881Z","shell.execute_reply.started":"2023-07-25T02:18:55.309601Z","shell.execute_reply":"2023-07-25T02:19:11.152553Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorch-pretrained-bert\n  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (1.23.5)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (1.26.100)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (4.65.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2023.6.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (1.29.161)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (2023.5.7)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch-pretrained-bert) (2.8.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch-pretrained-bert) (1.16.0)\nInstalling collected packages: pytorch-pretrained-bert\nSuccessfully installed pytorch-pretrained-bert-0.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\nimport torch.optim as optim\n \n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n#import logging\n#logging.basicConfig(level=logging.INFO)\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom argparse import Namespace\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm as tq\nfrom tqdm import notebook\n ","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:11.156685Z","iopub.execute_input":"2023-07-25T02:19:11.157110Z","iopub.status.idle":"2023-07-25T02:19:17.460567Z","shell.execute_reply.started":"2023-07-25T02:19:11.157070Z","shell.execute_reply":"2023-07-25T02:19:17.459553Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"dat = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndat","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:17.462134Z","iopub.execute_input":"2023-07-25T02:19:17.462852Z","iopub.status.idle":"2023-07-25T02:19:19.052851Z","shell.execute_reply.started":"2023-07-25T02:19:17.462814Z","shell.execute_reply":"2023-07-25T02:19:19.051781Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      One of the other reviewers has mentioned that ...  positive\n1      A wonderful little production. <br /><br />The...  positive\n2      I thought this was a wonderful way to spend ti...  positive\n3      Basically there's a family where a little boy ...  negative\n4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n...                                                  ...       ...\n49995  I thought this movie did a down right good job...  positive\n49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n49997  I am a Catholic taught in parochial elementary...  negative\n49998  I'm going to have to disagree with the previou...  negative\n49999  No one expects the Star Trek movies to be high...  negative\n\n[50000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>I thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>I am a Catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>I'm going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>No one expects the Star Trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Данные сбалансированы по классам.\n(dat.sentiment=='positive').sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.055976Z","iopub.execute_input":"2023-07-25T02:19:19.056637Z","iopub.status.idle":"2023-07-25T02:19:19.074714Z","shell.execute_reply.started":"2023-07-25T02:19:19.056599Z","shell.execute_reply":"2023-07-25T02:19:19.073481Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"25000"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_train, data_val = train_test_split(dat, test_size=0.20, random_state=42)\ndata_train['split']='train'\ndata_val['split']='val'\ndata_with_split=pd.concat([data_train, data_val], ignore_index=True)\ndata_with_split","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.076474Z","iopub.execute_input":"2023-07-25T02:19:19.076854Z","iopub.status.idle":"2023-07-25T02:19:19.106771Z","shell.execute_reply.started":"2023-07-25T02:19:19.076817Z","shell.execute_reply":"2023-07-25T02:19:19.105624Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment  split\n0      That's what I kept asking myself during the ma...  negative  train\n1      I did not watch the entire movie. I could not ...  negative  train\n2      A touching love story reminiscent of In the M...  positive  train\n3      This latter-day Fulci schlocker is a totally a...  negative  train\n4      First of all, I firmly believe that Norwegian ...  negative  train\n...                                                  ...       ...    ...\n49995  Although Casper van Dien and Michael Rooker ar...  negative    val\n49996  I liked this movie. I wasn't really sure what ...  positive    val\n49997  Yes non-Singaporean's can't see what's the big...  positive    val\n49998  As far as films go, this is likable enough. En...  negative    val\n49999  I saw Anatomy years ago -- dubbed at a friends...  positive    val\n\n[50000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>That's what I kept asking myself during the ma...</td>\n      <td>negative</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I did not watch the entire movie. I could not ...</td>\n      <td>negative</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A touching love story reminiscent of In the M...</td>\n      <td>positive</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This latter-day Fulci schlocker is a totally a...</td>\n      <td>negative</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>First of all, I firmly believe that Norwegian ...</td>\n      <td>negative</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>Although Casper van Dien and Michael Rooker ar...</td>\n      <td>negative</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>I liked this movie. I wasn't really sure what ...</td>\n      <td>positive</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>Yes non-Singaporean's can't see what's the big...</td>\n      <td>positive</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>As far as films go, this is likable enough. En...</td>\n      <td>negative</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>I saw Anatomy years ago -- dubbed at a friends...</td>\n      <td>positive</td>\n      <td>val</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.108436Z","iopub.execute_input":"2023-07-25T02:19:19.109160Z","iopub.status.idle":"2023-07-25T02:19:19.595130Z","shell.execute_reply.started":"2023-07-25T02:19:19.109118Z","shell.execute_reply":"2023-07-25T02:19:19.594146Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 231508/231508 [00:00<00:00, 2602489.70B/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class IMDBDataset(Dataset):\n    def __init__(self, IMDB_df, max_seq_length):\n        \"\"\"\n        Args:\n            IMDB_df (pandas.DataFrame): the dataset with bert_tokens\n            \n        \"\"\"\n        self.IMDB_df = IMDB_df \n        \n        self._max_seq_length = max_seq_length\n\n        self.train_df = self.IMDB_df[self.IMDB_df.split=='train']\n        self.train_size = len(self.train_df)\n\n        self.val_df = self.IMDB_df[self.IMDB_df.split=='val']\n        self.validation_size = len(self.val_df)\n\n        self._lookup_dict = {'train': (self.train_df, self.train_size), \n                             'val': (self.val_df, self.validation_size)}\n\n        self.set_split('train')\n       \n\n    def set_split(self, split=\"train\"):\n        self._data_split = split\n        self._data_df, self._data_size = self._lookup_dict[split]\n\n    def __len__(self):\n        return self._data_size\n\n    def __getitem__(self, index):\n        \n        row = self._data_df.iloc[index]\n        review=row['review']\n        tokens=tokenizer.tokenize(review)\n        if len(tokens)<self._max_seq_length:\n            token_index=tokenizer.convert_tokens_to_ids(tokens)\n            pad=[0]*(self._max_seq_length-len(tokens))\n            token_index=token_index+pad\n        else:\n            token_index=tokenizer.convert_tokens_to_ids(tokens[:self._max_seq_length])\n            \n        data_vector = torch.LongTensor(token_index) \n        \n        target = int(row.sentiment=='positive')\n        \n            \n\n        return {'x_data': data_vector, \n                'y_target': torch.LongTensor([target]),\n                'x_length': len(tokens)}\n\n    def get_num_batches(self, batch_size):\n        \n        return len(self) // batch_size\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.596678Z","iopub.execute_input":"2023-07-25T02:19:19.597301Z","iopub.status.idle":"2023-07-25T02:19:19.608915Z","shell.execute_reply.started":"2023-07-25T02:19:19.597264Z","shell.execute_reply":"2023-07-25T02:19:19.608058Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data=IMDBDataset(data_with_split, 256)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.612414Z","iopub.execute_input":"2023-07-25T02:19:19.613891Z","iopub.status.idle":"2023-07-25T02:19:19.652724Z","shell.execute_reply.started":"2023-07-25T02:19:19.613854Z","shell.execute_reply":"2023-07-25T02:19:19.651891Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"(data.__getitem__(2)).items()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.654494Z","iopub.execute_input":"2023-07-25T02:19:19.655213Z","iopub.status.idle":"2023-07-25T02:19:19.736838Z","shell.execute_reply.started":"2023-07-25T02:19:19.655178Z","shell.execute_reply":"2023-07-25T02:19:19.735911Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"dict_items([('x_data', tensor([ 1037,  7244,  2293,  2466, 14563,  1997,  1999,  1996,  6888,  2005,\n         2293,  1005,  1012,  5059,  4600,  2006,  2822,  4623,  1998,  2129,\n         2023,  2003,  2109,  2011,  2789,  2111,  2000, 10639,  5346,  2000,\n         2169,  2060,  1010,  1996,  2466,  7679,  2006,  1037,  2082, 24741,\n         2040,  4122,  2061,  2172,  2000,  2022,  1037,  2944,  3836,  2004,\n         2092,  2004,  1037,  2204,  3129,  1998,  2269,  1012,  1037,  3026,\n         3076,  2003,  2200,  6296,  2000,  2032,  1012,  2004,  1996,  2466,\n         4895, 10371,  2015,  2057,  2156,  1996,  6699,  2917,  1996,  3302,\n         1999,  2010,  2322,  2095,  3510,  1998,  2129,  2002, 24665, 23804,\n         2015,  2007,  1996,  7191, 21883,  2015,  2008,  2227,  2032,  1012,\n         1037,  3376,  1998,  3048,  2466,  1012,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0])), ('y_target', tensor([1])), ('x_length', 106)])"},"metadata":{}}]},{"cell_type":"code","source":"def generate_batches(dataset, batch_size, shuffle=True,\n                     drop_last=True, device=\"cpu\"): \n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will \n      ensure each tensor is on the write device location.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n                            shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in dataloader:\n        out_data_dict = {}\n        for name, tensor in data_dict.items():\n            out_data_dict[name] = data_dict[name].to(device)\n            \n        yield out_data_dict","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.740925Z","iopub.execute_input":"2023-07-25T02:19:19.741199Z","iopub.status.idle":"2023-07-25T02:19:19.747263Z","shell.execute_reply.started":"2023-07-25T02:19:19.741174Z","shell.execute_reply":"2023-07-25T02:19:19.746176Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"gen=generate_batches(data, 16)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.748997Z","iopub.execute_input":"2023-07-25T02:19:19.749331Z","iopub.status.idle":"2023-07-25T02:19:19.764452Z","shell.execute_reply.started":"2023-07-25T02:19:19.749298Z","shell.execute_reply":"2023-07-25T02:19:19.763443Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"next(gen)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.765840Z","iopub.execute_input":"2023-07-25T02:19:19.766325Z","iopub.status.idle":"2023-07-25T02:19:19.967164Z","shell.execute_reply.started":"2023-07-25T02:19:19.766282Z","shell.execute_reply":"2023-07-25T02:19:19.966091Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'x_data': tensor([[ 1045,  2245,  8909,  ...,  2187,  2077,  1996],\n         [ 1045,  2031,  2000,  ...,  2572,  2025,  1037],\n         [ 1045,  2387,  1062,  ...,  2013,  1996,  4378],\n         ...,\n         [ 1996, 13972,  3727,  ...,     0,     0,     0],\n         [ 3172,  2001,  1037,  ...,  1997,  1037,  2843],\n         [ 2111,  3422,  5691,  ...,  1005, 11598,  2015]]),\n 'y_target': tensor([[0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1]]),\n 'x_length': tensor([512, 381, 791, 276, 167, 418, 268, 255, 158, 189, 160, 752, 380,  63,\n         628, 335])}"},"metadata":{}}]},{"cell_type":"code","source":"class BertForSequenceClassification(nn.Module):\n  \n    def __init__(self, num_labels=1):\n        super(BertForSequenceClassification, self).__init__()\n        self.num_labels = num_labels \n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(config.hidden_dropout_prob) \n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        nn.init.xavier_normal_(self.classifier.weight)\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False) \n        pooled_output = self.dropout(pooled_output) \n        logits = self.classifier(pooled_output)\n        \n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.968859Z","iopub.execute_input":"2023-07-25T02:19:19.969213Z","iopub.status.idle":"2023-07-25T02:19:19.977062Z","shell.execute_reply.started":"2023-07-25T02:19:19.969180Z","shell.execute_reply":"2023-07-25T02:19:19.975969Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"BertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:19.978635Z","iopub.execute_input":"2023-07-25T02:19:19.979539Z","iopub.status.idle":"2023-07-25T02:19:34.934682Z","shell.execute_reply.started":"2023-07-25T02:19:19.979513Z","shell.execute_reply":"2023-07-25T02:19:34.933682Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 407873900/407873900 [00:08<00:00, 48785224.63B/s]\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): BertLayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): BertLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"config=Namespace(\nhidden_dropout_prob=0.1,\nhidden_size=768\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:34.936050Z","iopub.execute_input":"2023-07-25T02:19:34.936486Z","iopub.status.idle":"2023-07-25T02:19:34.943388Z","shell.execute_reply.started":"2023-07-25T02:19:34.936450Z","shell.execute_reply":"2023-07-25T02:19:34.942397Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model=BertForSequenceClassification()\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:34.946430Z","iopub.execute_input":"2023-07-25T02:19:34.946730Z","iopub.status.idle":"2023-07-25T02:19:40.958071Z","shell.execute_reply.started":"2023-07-25T02:19:34.946686Z","shell.execute_reply":"2023-07-25T02:19:40.956853Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def compute_accuracy(y_prd, y_trgt, device):\n    y_trgt=y_trgt.to(device)\n        \n    preds = (torch.sigmoid(y_prd)>=0.5).to(torch.LongTensor()).to(device)\n    all_cor=torch.sum(preds*y_trgt, dim=0)\n    \n    return all_cor/torch.sum(y_trgt, dim=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:40.959673Z","iopub.execute_input":"2023-07-25T02:19:40.960288Z","iopub.status.idle":"2023-07-25T02:19:40.967122Z","shell.execute_reply.started":"2023-07-25T02:19:40.960248Z","shell.execute_reply":"2023-07-25T02:19:40.965772Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"args = Namespace(\n    # Training hyper parameter\n    num_epochs=5,\n    learning_rate=1e-3,\n    batch_size=16,\n    seed=1337,\n    #early_stopping_criteria=5,\n    cuda=True\n    )","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:40.968750Z","iopub.execute_input":"2023-07-25T02:19:40.969320Z","iopub.status.idle":"2023-07-25T02:19:41.136421Z","shell.execute_reply.started":"2023-07-25T02:19:40.969284Z","shell.execute_reply":"2023-07-25T02:19:41.135317Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def make_train_state(args):\n    return {'stop_early': False,\n            #'early_stopping_step': 0,\n            #'early_stopping_best_val': 1e8,\n            'learning_rate': args.learning_rate,\n            'epoch_index': 0,\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': []\n            }\n            ","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:41.138430Z","iopub.execute_input":"2023-07-25T02:19:41.138846Z","iopub.status.idle":"2023-07-25T02:19:41.148992Z","shell.execute_reply.started":"2023-07-25T02:19:41.138809Z","shell.execute_reply":"2023-07-25T02:19:41.147994Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"make_train_state(args)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:41.150665Z","iopub.execute_input":"2023-07-25T02:19:41.151060Z","iopub.status.idle":"2023-07-25T02:19:41.167884Z","shell.execute_reply.started":"2023-07-25T02:19:41.151025Z","shell.execute_reply":"2023-07-25T02:19:41.166698Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'stop_early': False,\n 'learning_rate': 0.001,\n 'epoch_index': 0,\n 'train_loss': [],\n 'train_acc': [],\n 'val_loss': [],\n 'val_acc': []}"},"metadata":{}}]},{"cell_type":"code","source":"# Check CUDA\nif not torch.cuda.is_available():\n    args.cuda = False\n\nargs.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n    \nprint(\"Using CUDA: {}\".format(args.cuda))","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:41.169557Z","iopub.execute_input":"2023-07-25T02:19:41.170396Z","iopub.status.idle":"2023-07-25T02:19:41.247363Z","shell.execute_reply.started":"2023-07-25T02:19:41.170361Z","shell.execute_reply":"2023-07-25T02:19:41.246532Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Using CUDA: True\n","output_type":"stream"}]},{"cell_type":"code","source":"args.device","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:41.250910Z","iopub.execute_input":"2023-07-25T02:19:41.252021Z","iopub.status.idle":"2023-07-25T02:19:41.262713Z","shell.execute_reply.started":"2023-07-25T02:19:41.251931Z","shell.execute_reply":"2023-07-25T02:19:41.261754Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"Обучение и валидация модели.","metadata":{}},{"cell_type":"code","source":"classifier = model.to(args.device)    \n#loss_func = nn.CrossEntropyLoss()\nloss_func=torch.nn.BCEWithLogitsLoss()\n#optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\nlrlast = .001\nlrmain = .00001\noptimizer = optim.Adam(\n    [\n        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n       \n   ])\n\nsig=nn.Sigmoid()\n                                \n\ntrain_state = make_train_state(args)\n\nepoch_bar = notebook.tqdm(desc='training routine', \n                          total=args.num_epochs,\n                          position=0)\n\ndata.set_split('train')\ntrain_bar = notebook.tqdm(desc='split=train',\n                          total=data.get_num_batches(args.batch_size), \n                          position=1, \n                          leave=True)\ndata.set_split('val')\nval_bar = notebook.tqdm(desc='split=val',\n                        total=data.get_num_batches(args.batch_size), \n                        position=1, \n                        leave=True)\n\ntry:\n    for epoch_index in range(args.num_epochs):\n        train_state['epoch_index'] = epoch_index\n\n        # Iterate over training dataset\n\n        # setup: batch generator, set loss and acc to 0, set train mode on\n        data.set_split('train')\n        batch_generator = generate_batches(data, \n                                           batch_size=args.batch_size, \n                                           device=args.device)\n        running_loss = 0.0\n        running_acc = 0.0\n        classifier.train()\n\n        for batch_index, batch_dict in enumerate(batch_generator):\n            n_batch=batch_index\n            # the training routine is these 5 steps:\n\n            # --------------------------------------    \n            # step 1. zero the gradients\n                        \n            optimizer.zero_grad()\n\n            # step 2. compute the output\n            y_pred = classifier(batch_dict['x_data'])\n            \n            # step 3. compute the loss\n            \n            loss = loss_func(y_pred, batch_dict['y_target'].float())\n    \n            #running_loss += (loss.item() - running_loss) / (batch_index + 1)\n            running_loss += loss.item()\n\n            # step 4. use loss to produce gradients\n            loss.backward()\n\n            # step 5. use optimizer to take gradient step\n            optimizer.step()\n            # -----------------------------------------\n            # compute the accuracy\n            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], args.device)\n            #running_acc += (acc_t - running_acc) / (batch_index + 1)\n            running_acc += acc_t\n\n            # update bar\n            train_bar.set_postfix(loss=running_loss/(batch_index + 1), acc=running_acc/(batch_index + 1), epoch=epoch_index)\n            train_bar.update()\n\n        train_state['train_loss'].append(running_loss/(n_batch+1))\n        train_state['train_acc'].append(running_acc/(n_batch+1))\n\n        # Iterate over val dataset\n\n        # setup: batch generator, set loss and acc to 0; set eval mode on\n\n        data.set_split('val')\n        batch_generator = generate_batches(data, \n                                           batch_size=args.batch_size, \n                                           device=args.device)\n        running_loss = 0.\n        running_acc = 0.\n        classifier.eval()\n        \n\n        for batch_index, batch_dict in enumerate(batch_generator):\n            n_batch=batch_index\n            # compute the output\n            y_pred = classifier(batch_dict['x_data'])\n            \n            # step 3. compute the loss\n            loss = loss_func(y_pred, batch_dict['y_target'].float())\n            #running_loss += (loss.item() - running_loss) / (batch_index + 1)\n            running_loss += loss.item()\n            # compute the accuracy\n            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], args.device)\n            #running_acc += (acc_t - running_acc) / (batch_index + 1)\n            running_acc += acc_t\n            \n            val_bar.set_postfix(loss=running_loss/(batch_index + 1), acc=running_acc/(batch_index + 1), epoch=epoch_index)\n            val_bar.update()\n\n        train_state['val_loss'].append(running_loss/(n_batch+1))\n        train_state['val_acc'].append(running_acc/(n_batch+1))\n\n        train_bar.n = 0\n        val_bar.n = 0\n        epoch_bar.update()\n\n        if train_state['stop_early']:\n            break\n            \nexcept KeyboardInterrupt:\n    print(\"Exiting loop\")","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:41.264286Z","iopub.execute_input":"2023-07-25T02:19:41.264669Z","iopub.status.idle":"2023-07-25T04:26:54.075842Z","shell.execute_reply.started":"2023-07-25T02:19:41.264635Z","shell.execute_reply":"2023-07-25T04:26:54.074672Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"training routine:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e61ceaa32f2641f087fce6fd863de071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"split=train:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3261721b2ec14d4da27b84debbb9b579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"split=val:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e473cfa0395345f4a4ff0747ccfe8595"}},"metadata":{}},{"name":"stdout","text":"Exiting loop\n","output_type":"stream"}]},{"cell_type":"code","source":"train_state['val_acc']","metadata":{"execution":{"iopub.status.busy":"2023-07-25T04:26:54.077618Z","iopub.execute_input":"2023-07-25T04:26:54.078676Z","iopub.status.idle":"2023-07-25T04:26:54.089209Z","shell.execute_reply.started":"2023-07-25T04:26:54.078638Z","shell.execute_reply":"2023-07-25T04:26:54.087965Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[tensor([0.9211], device='cuda:0'),\n tensor([0.9143], device='cuda:0'),\n tensor([0.9320], device='cuda:0')]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}